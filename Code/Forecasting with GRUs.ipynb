{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "113ifxUBsLOI"
   },
   "source": [
    "# Solving a temperature-forecasting problem with GRUs\n",
    "\n",
    "So far we've mainly discussed sequence data in the context of textual data, such as the IMDB dataset. However, sequence data is applicable to a wide range of problems beyond language processing. In the following examples, we'll look at a weather time series dataset collected at the weather station of the Max Planck Institute for Biogeochemistry in Jena, Germany.\n",
    "\n",
    "This dataset includes 14 different variables such as air temperature, air pressure, humidity, wind direction and others, recorded every 10 minutes over several years. While the original data go back to 2003, our focus here is on the data from 2009 to 2016. Our goal is to construct a model that takes recent historical data (a few days' worth of data points) as input and predicts the air temperature 24 hours into the future.\n",
    "\n",
    "You can obtain and decompress the data using the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlH2dZ42jXdv",
    "outputId": "f7d79432-b18e-4c35-f5f4-b218338aba36"
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#mkdir jena_climate\n",
    "#cd jena_climate\n",
    "#wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
    "#unzip jena_climate_2009_2016.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSks8BzYuqQy"
   },
   "source": [
    "By the way, the following code is a modified version of the code that can be found in [1]. That said, let us begin by importing some useful libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-pHurD3jcFs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR1fLZdEwIaB"
   },
   "source": [
    "Let's examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txuUYEhnrN2t",
    "outputId": "1d71e4f7-ec84-492f-bc5a-3951f9c305c2"
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/dotero/Documents/TEC/Cursos/Bloque Integrador/2024/MA3001B/Code/jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G4roi_3wQqU"
   },
   "source": [
    "We have a total of 420,551 lines of data (each line representing a time step with a date) and 14 weather-related measurements, plus the header. Now we will turn this data into a `NumPy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYrw_oQqjv7i"
   },
   "outputs": [],
   "source": [
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRTNJpy3xTvQ"
   },
   "source": [
    "Here is the plot of temperature (in degrees Celsius) over time. On this plot, you can clearly see the yearly periodicity of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "Csp24DN9lMSV",
    "outputId": "b4430c71-62f0-4885-a333-8021cf560f7a"
   },
   "outputs": [],
   "source": [
    "temp = float_data[:, 1]\n",
    "plt.plot(range(len(temp)), temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DdPEfHbzISE"
   },
   "source": [
    "## Formulation of the problem\n",
    "\n",
    "The problem we will be solving with GRUs goes as follows: given data going as far back as `lookback` timesteps (a timestep is 10 minutes) and sampled every `steps` timesteps, can we predict the temperature in `delay` timesteps? We will use the following parameter values:\n",
    "\n",
    "- `lookback`: 720—Observations will go back 5 days.\n",
    "- `steps`: 6—Observations will be sampled at one data point per hour.\n",
    "- `delay`: 144—Targets will be 24 hours in the future.\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "This task is straightforward because the data is already numerical, so no need for vectorization. However, each time series in the dataset operates on a different scale: for example, temperature typically ranges between -20 and +30, whereas atmospheric pressure, measured in mbar, hovers around 1,000. To deal with this, we'll normalize each time series independently, ensuring that they all have small values within a comparable scale.\n",
    "\n",
    "We’ll preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation. We’re going to use the first 200,000 timesteps as training data, so we will compute the mean and standard deviation only on this fraction of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfK6n6DllQfQ"
   },
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-St9D8kB070c"
   },
   "source": [
    "We will also need a `Python` generator that takes the current float data array and produces batches of historical data along with a target temperature for the future. Due to the high redundancy between samples in the dataset (where sample $n$ and sample $n+1$ share many timesteps), it's inefficient to allocate memory explicitly for each sample. Instead, you'll dynamically generate samples as needed using the original data.\n",
    "\n",
    "The generator returns a tuple (`samples`, `targets`), where `samples` is a batch of input data and `targets` is the corresponding array of target temperatures. It takes the following arguments:\n",
    "\n",
    "- `data`: The original array of floating-point data after normalization.\n",
    "- `lookback`: How many timesteps back the input data should go.\n",
    "- `delay`: How many timesteps in the future the target should be.\n",
    "- `min_index` and `max_index`: Indices in the data array that delimit which time-steps to draw from. This is useful for keeping a segment of the data for validation and another for testing.\n",
    "- `shuffle`: Whether to shuffle the samples or draw them in chronological order.\n",
    "- `batch_size`: The number of samples per batch.\n",
    "- `step`: The period, in timesteps, at which you sample data It’ll set it to 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1RSLBaelsZR"
   },
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNz5Y8A64Kvc"
   },
   "source": [
    "Now let's use the generic generator function to create three specific generators: one for training, one for validation, and one for testing. Each generator will focus on different time segments of the original data: the training generator will process the first 200,000 time steps, the validation generator will process the next 100,000 time steps, and the test generator will process the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMhWKl2Km3Q6"
   },
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data, lookback=lookback, delay=delay, min_index=0,\n",
    "                      max_index=200000, shuffle=True, step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data, lookback=lookback, delay=delay, min_index=200001,\n",
    "                    max_index=300000, step=step, batch_size=batch_size)\n",
    "test_gen = generator(float_data, lookback=lookback, delay=delay, min_index=300001,\n",
    "                     max_index=None, step=step, batch_size=batch_size)\n",
    "\n",
    "# How many steps to draw from val_gen in order to see the entire validation set\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "# How many steps to draw from test_gen in order to see the entire test set\n",
    "test_steps = (len(float_data) - 300001 - lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45sGyrTl4zMw"
   },
   "source": [
    "## A first recurrent baseline\n",
    "\n",
    "We'll experiment with a recurrent sequence processing model, which is ideal for the type of data we're working with.\n",
    "\n",
    "We are going to use the GRU layer developed by Chung et al. in 2014. Gated Recurrent Unit (GRU) layers work on the same principle as LSTMs, but are more streamlined and therefore less computationally intensive (although they may not offer the same level of representational capacity as LSTMs). This trade-off between computational efficiency and representational capacity is a common consideration in various aspects of machine learning.\n",
    "\n",
    "By the way, note that we'll be using the **mean absolute error**, also knows as **MAE**, as our evaluation metric. This is one of many choices for regression and forecasting applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkM0iVbYnOZN",
    "outputId": "873d4ed3-e2f5-4942-f4b1-b07e36d0bb94"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(32))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit(train_gen, steps_per_epoch=500, epochs=20,\n",
    "                    validation_data=val_gen, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29Eu38ba7aHR"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srcW3ybv6aBh"
   },
   "source": [
    "The validation MAE of ~0.265 (before overfitting) translates to a mean absolute error of 2.35 ̊C after denormalization. It's good, but we have a bit of a margin for improvement.\n",
    "\n",
    "## Using recurrent dropout to fight overfitting\n",
    "\n",
    "The training and validation curves clearly indicate that the model is experiencing overfitting, as evidenced by a significant divergence between their respective losses after a few epochs. You're already familiar with a traditional method to combat this issue: dropout, which randomly deactivates input units of a layer to break any incidental correlations in the training data that the layer might learn. However, applying dropout correctly in recurrent networks poses a non-trivial challenge. It's been long recognized that using dropout before a recurrent layer can hinder learning instead of aiding in regularization.\n",
    "\n",
    "In 2015, Yarin Gal, as part of his PhD thesis on Bayesian deep learning, established the proper method for employing dropout in a recurrent network: applying the same dropout mask (i.e., the same pattern of deactivated units) at every timestep, rather than using a randomly varying dropout mask from timestep to timestep. Furthermore, to regularize the representations formed by the recurrent gates of layers like GRU and LSTM, a temporally consistent dropout mask should be applied to the inner recurrent activations of the layer (known as recurrent dropout). Employing the same dropout mask at each timestep enables the network to effectively propagate its learning errors over time, whereas a temporally random dropout mask would disrupt this error signal and impede the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67GzBX_p8-yo"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500, epochs=10,\n",
    "                                      validation_data=val_gen,\n",
    "                                      validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsAEo8ym9UVM"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQopFne89XAU"
   },
   "source": [
    "The overfitting problem has been solved, however, the best scores aren’t much lower than they were previously.\n",
    "\n",
    "## Stacking recurrent layers\n",
    "\n",
    "Since we've managed to address the overfitting issue but are now facing a performance bottleneck, it might be beneficial to enhance the network's capacity. Remember the universal machine-learning workflow, which suggests increasing network capacity until overfitting becomes the main challenge (assuming you're already employing basic techniques like dropout to mitigate overfitting). If you're not struggling with severe overfitting, it's likely that the network is under capacity.\n",
    "\n",
    "Boosting network capacity usually involves adding more units to the layers or incorporating additional layers. One classic method to create more powerful recurrent networks is by stacking recurrent layers.\n",
    "\n",
    "In Keras, to stack recurrent layers on top of each other, all intermediate layers should return their complete sequence of outputs (a 3D tensor) rather than just the output at the last timestep. This can be achieved by setting `return_sequences=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOG8FTHLov-x"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True))\n",
    "model.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit(train_gen, steps_per_epoch=500, epochs=10,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE1CdHKd-h1A"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Rfy7zC-qtv"
   },
   "source": [
    "The inclusion of an extra layer does lead to a slight enhancement in the results, although the improvement is not particularly significant. Two key points can be inferred:\n",
    "\n",
    "- Since we're still not experiencing severe overfitting, there is room to increase the size of the layers in an attempt to improve validation loss. However, this comes with a noticeable increase in computational demands.\n",
    "\n",
    "- The addition of another layer did not result in a substantial improvement, indicating that we may be encountering diminishing returns from expanding the network's capacity further at this stage.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Chollet, Francois. *Deep learning with Python*. Simon and Schuster, 2021."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
