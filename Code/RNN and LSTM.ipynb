{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6WXJfFKWUHf"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Let us see how a simple RNN performs doing sentiment analisys on the IMDB dataset. As usual, let us import some modules and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqQUKdU-WRe2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvi5Y-TBHCD-"
   },
   "source": [
    "## Processing text data\n",
    "\n",
    "Deep learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels.\n",
    "\n",
    "As expected, deep-learning models do not take as input raw text, they only work with numeric tensors. To deal with this we need to **vectorize** text, which is the process of transforming text into numeric tensors. This can be done in multiple ways:\n",
    "\n",
    "- Segment text into words, and transform each word into a vector.\n",
    "- Segment text into characters, and transform each character into a vector.\n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "\n",
    "The different units into which you can break down text (words, characters, or n-grams) are called **tokens**, and breaking text into such tokens is called **tokenization**. In general, text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens. These vectors are fed into deep neural networks. There are multiple ways to associate a vector with a token. Let us talk about two major ones: **one-hot encoding** of tokens, and **token embedding**.\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding is the most common, most basic way to turn a token into a vector. It consists of associating a unique integer index with every word and then turning this integer index $i$ into a binary vector of size $n$, which is the size of the vacabulary: the vector is all zeros except for the i-th entry, which is equal to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woV8IMLt-JUr"
   },
   "source": [
    "The type of RNN we will implement is *many-to-one*. The idea is to predict if a movie review is *positive* or *negative*. The data we will work with is the **IMDB** database that is already included in `keras`. For now, let us import the data limiting the number of the most frequent words we will handle (`max_features`), and also truncating, or padding, the reviews so that they have `maxlen` words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c06-SSD8HGxk",
    "outputId": "24a321db-c3c2-47fe-ca12-6253da15c964"
   },
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt01DB-gHIJi"
   },
   "source": [
    "### Token embedding\n",
    "\n",
    "This method is mainly used for words, so it is also known as **word embeddings**. Whereas the vectors obtained through one-hot encoding are binary, sparse, and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are low-dimensional floating-point vectors: that is, dense vectors, as opposed to sparse vectors. So, word embeddings pack more information into far fewer dimensions.\n",
    "\n",
    "There are two ways to obtain word embeddings: **learn word embeddings** jointly with the main task you care about, and load **pretrained word embeddings**.\n",
    "\n",
    "#### Learning word embeddings\n",
    "\n",
    "The simplest way to associate a dense vector with a word is to choose the vector at random. The problem with this approach is that the resulting embedding space has no structure. Something more useful would be to have a space in which the geometric relationships between word vectors should reflect the semantic relationships between these words. Then, word embeddings are meant to map human language into a geometric space.\n",
    "\n",
    "In real-world word-embedding spaces, common examples of meaningful geometric transformations are **gender** vectors and **plural** vectors. For instance, by adding a `female` vector to the vector `king`, we obtain the vector `queen`. By adding a `plural` vector, we obtain `kings`.Word-embedding spaces typically feature thousands of such interpretable and potentially useful vectors.\n",
    "\n",
    "Learning a word embedding of a particular task is equivalent to training an extra layer of a neural network, which is known as the `Embedding` layer. This layer can be understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. By the way, this layer needs to be told, at least, the size of the vocabulary we are working with and the dimensionality of the embedding.\n",
    "\n",
    "To see how this works, let us implement a classifiers that predicts if a movie review is *positive* or *negative*. The data we will work with is the **IMDB** database that is already included in `keras`. For now, let us import the data limiting the number of the most frequent words we will handle (`max_features`), and also truncating the reviews so that they have twenty words at most (`maxlen`).\n",
    "\n",
    "#### Pretrained word embeddings\n",
    "\n",
    "When little training data is available, instead of learning word embeddings jointly with the problem we want to solve, we can load embedding vectors from a precomputed embedding space that we know is highly structured and exhibits useful properties. This is analogous to the concept of transfer learning: there is not enough data available to learn truly powerful features on our own, but we expect the features that we need to be fairly generic.\n",
    "\n",
    "The idea of a dense, low-dimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the **Word2vec** algorithm, developed by Tomas Mikolov at Google in 2013. Word2vec dimensions capture specific semantic properties, such as gender.\n",
    "\n",
    "There are various precomputed databases of word embeddings that you can download and use in a `keras` embedding layer. Word2vec is one of them. Another popular one is called **Global Vectors for Word Representation (GloVe)**, which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOZH-WbqHNBZ"
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "The type of RNN we will implement is *many-to-one*. The idea is to predict if a movie review is *positive* or *negative*. The data we will work with is the **IMDB** database that is already included in `keras`. For now, let us import the data limiting the number of the most frequent words we will handle (`max_features`), and also truncating, or padding, the reviews so that they have `maxlen` words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUPAmNmVVd6u",
    "outputId": "7c143165-2c46-41d7-b936-9d0183806f9e"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 500\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = tf.keras.preprocessing.sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = tf.keras.preprocessing.sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KxadjnXO7v3"
   },
   "source": [
    "We will build a neural network that will have a `SimpleRNN` layer on top of the `Embedding` layer. In this case, the dimensionality of the embedding space is 32, which means that the dimensionality of the input of the `SimpleRNN` layer is also 32. Notice that this is also the dimensionality of the output of the `SimpleRNN` layer.\n",
    "\n",
    "By the way, regarding the number of parameters of the `SimpleRNN` layer, the number of trainable parameters of this layer is equal to\n",
    "\n",
    "$$\\dim(W_{aa})+\\dim(W_{ax})+\\dim(b_a).$$\n",
    "\n",
    "In this case, we have that\n",
    "\n",
    "$$32\\times32+32\\times32+32\\times1=2080.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lv-gzp2t5VOM",
    "outputId": "9a848564-86c3-40f3-ca62-3e0bd0661906"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Embedding(max_features, 32),\n",
    "                    SimpleRNN(32),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "                   ])\n",
    "model.build(input_shape=(None, None))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-iI7exlYTim"
   },
   "source": [
    "Let us compile the model. Notice that we are doing binary classification, so the cost function must be `binary_crossentropy`. The evaluation metric will be *accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jxiif45o5iUj"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO-l5OsSYkBz"
   },
   "source": [
    "And now we train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNmpLY5e5iL6",
    "outputId": "8df47f62-e3d7-4ade-b0a9-d09498c3ce80"
   },
   "outputs": [],
   "source": [
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QEcpt4Uc3Ej"
   },
   "source": [
    "We get an accuracy of about 80% on the validation set, but we are overfitting the data a bit. Keep in mind that RNNs do not behave that well with long sequences, which could explain in part why the validation accuracy is not higher. Let us visualize these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "A1yBg-d65iB_",
    "outputId": "80744538-a444-4fd3-ba1c-80dd17e97741"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYw4CSBVmP_C"
   },
   "source": [
    "## \"Deep\" RNNs\n",
    "\n",
    "It is worthwhile to mention that several `SimpleRNN` layers can be stacked on top of each other in order to increase the representational power of a network.\n",
    "\n",
    "Like all recurrent layers in Keras, `SimpleRNN` can be run in two different modes: it can return either the full sequences of successive outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`), or only the last output for each input sequence (a 2D tensor of shape `(batch_size, output_features)`). These two modes are controlled by the `return_sequences` constructor argument. If we stack `SimpleRNN` layers, all the intermediate `SimpleRNN` layers must have this parameter set to `True`.\n",
    "\n",
    " Let us build a model with an extra `SimpleRNN` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hF4TN8IpLSr",
    "outputId": "dd0fd5b4-e0e4-4d25-d1a4-caef65b714dd"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Embedding(max_features, 32),\n",
    "                    SimpleRNN(32, return_sequences=True),\n",
    "                    SimpleRNN(32),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "                   ])\n",
    "model.build(input_shape=(None, None))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh6uNQPCpcXD"
   },
   "source": [
    "Let us see how this model performs on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqkM6IUgplpx",
    "outputId": "0353e6a3-e991-4913-9ffb-2762af5ec3fc"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj9JSjIjj1Rn"
   },
   "source": [
    "As expected, we are still having overfitting. This is due to the fact that we increased the complexity of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckKK0u-kgYnU"
   },
   "source": [
    "## LSTM\n",
    "\n",
    "`SimpleRNN` is not the only recurrent layer available in Keras. Two other options are `LSTM` and `GRU`. In practical applications, you will usually choose one of these, as `SimpleRNN` tends to be too basic to be of much use. `SimpleRNN` faces a key problem: despite its theoretical ability to retain information from time $t$ related to inputs seen far back in time, in practice it struggles to learn such long-term dependencies. This challenge arises from the *vanishing gradient problem*, similar to that observed in deep feedforward networks, where adding layers can render the network untrainable. Hochreiter, Schmidhuber and Bengio studied the theoretical foundations of this problem in the early 1990s. The `LSTM` and `GRU` layers were designed specifically to address this problem.\n",
    "\n",
    "Let us look at the `LSTM` layer. Hochreiter and Schmidhuber introduced the **Long Short-Term Memory** (`LSTM`) algorithm in 1997 as the culmination of their work on the vanishing gradient problem. This layer is an evolution of the `SimpleRNN` layer, with a mechanism for retaining information over many time steps. Think of it as a conveyor belt that runs parallel to the sequence you are analysing. Information from the sequence can jump onto this conveyor at any point, travel to a later timestep, and then be retrieved intact when needed. In essence, `LSTM` stores information for later use, preventing older signals from being lost during processing.\n",
    "\n",
    "Now let us set up the same sentiment analysis application we implemented before, but this time using an `LSTM` layer. The network is very similar to the one with the `SimpleRNN` layer that was just presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuFFs_zegYnU",
    "outputId": "c74d866a-5f8b-431c-8450-97ff6ce552df"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.build(input_shape=(None, None))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "pS1cR_tegYnU",
    "outputId": "aeea6600-3a64-49fe-9d8e-970c4f469cc9"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6DiVjzUqHXN"
   },
   "source": [
    "This time we achieved a validation accuracy of about 89%, much better than the `SimpleRNN` network. The main reason for this is that `LSTM` suffers much less from the vanishing gradient problem.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Chollet, Francois. *Deep learning with Python*. Simon and Schuster, 2021.\n",
    "\n",
    "[2] https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "\n",
    "[3] https://www.v7labs.com/blog/recurrent-neural-networks-guide"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
