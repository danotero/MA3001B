{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS-wWjsfWlec"
   },
   "source": [
    "# Time series forecasting with 1D CNN and LSTM\n",
    "\n",
    "Once again, we'll work with the weather time series dataset collected at the weather station of the Max Planck Institute for Biogeochemistry in Jena, Germany.\n",
    "\n",
    "Keep in mind that dataset includes 14 different variables such as air temperature, air pressure, humidity, wind direction and others, recorded every 10 minutes over several years. While the original data go back to 2003, our focus here is on the data from 2009 to 2016. Our goal is to construct a model that takes recent historical data (a few days' worth of data points) as input and predicts the air temperature 24 hours into the future.\n",
    "\n",
    "You can obtain and decompress the data using the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BwJA25KWlec",
    "outputId": "1a6df6d5-930e-405e-d651-26eba2469d06"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir jena_climate\n",
    "cd jena_climate\n",
    "wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
    "unzip jena_climate_2009_2016.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSx_YccHWled"
   },
   "source": [
    "By the way, the following code is a modified version of the code that can be found in [1]. That said, let us begin by importing some useful libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W8aBhMJZ5lE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra6c0_-HZ5lE"
   },
   "source": [
    "The following lines of code will open the `csv` file that was downloaded and store that information in the variable named `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LB7vqC3SWled",
    "outputId": "557e562a-8bdb-40ad-c62d-897d153c9826"
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/dotero/Documents/TEC/Cursos/Bloque Integrador/MA3001B/Code/jena_climate'\n",
    "fname = os.path.join(data_dir, \"jena_climate_2009_2016.csv\")\n",
    "\n",
    "with open(fname) as f:\n",
    "    data = f.read()\n",
    "\n",
    "lines = data.split(\"\\n\")\n",
    "header = lines[0].split(\",\")\n",
    "lines = lines[1:]\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5ASAQEFZ5lF"
   },
   "source": [
    "We can see that we're working with 14 variables and 420,551 registers. In the following cell, we'll create two `NumPy` arrays: one for the temperature variable (`temperature`), and another one for all the data (`raw_data`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOprgXR3Wled"
   },
   "outputs": [],
   "source": [
    "temperature = np.zeros((len(lines),))\n",
    "raw_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(\",\")[1:]]\n",
    "    temperature[i] = values[1]\n",
    "    raw_data[i, :] = values[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0X04lh2GWlee"
   },
   "source": [
    "Here is the plot of temperature (in degrees Celsius) over time. On this plot, you can clearly see the yearly periodicity of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "y2ewHkpyWlee",
    "outputId": "f0c682ee-a84a-4531-dd90-0f90bc7bd3d8"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(temperature)), temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0S2ye1WWlee"
   },
   "source": [
    "By the way, it is recommended that you always look for periodicity in your data. Time series data usually have periodicity on different time scales, so it's important to find these patterns so that the models we train can learn them.\n",
    "\n",
    "In the next steps we'll allocate 50% of the data to training, 25% to validation and the remainder to testing. When dealing with time series data, it's important to ensure that the validation and test data sets are more recent than the training data. This is because our goal is to predict the future based on past observations, not to predict the past based on the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wInAPj2QWlee",
    "outputId": "e1dd33e3-01b8-4cff-e39d-839dbed8ac36"
   },
   "outputs": [],
   "source": [
    "num_train_samples = int(0.5 * len(raw_data))\n",
    "num_val_samples = int(0.25 * len(raw_data))\n",
    "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
    "print(\"num_train_samples:\", num_train_samples)\n",
    "print(\"num_val_samples:\", num_val_samples)\n",
    "print(\"num_test_samples:\", num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOoizdpSWlef"
   },
   "source": [
    "## Formulation of the problem\n",
    "\n",
    "The problem we will be solving goes as follows: given data going as far back as `sequence_length` timesteps (a timestep is 10 minutes) and sampled every `sampling_rate` timesteps, can we predict the temperature in `delay` timesteps? We will use the following parameter values:\n",
    "\n",
    "- `sequence_length`: 120—Observations will go back 5 days.\n",
    "- `sampling_rate`: 6—Observations will be sampled at one data point per hour.\n",
    "- `delay`: Targets will be 24 hours after the end of the sequence.\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "Each time series in the dataset operates on a different scale: for example, temperature typically ranges between -20 and +30, whereas atmospheric pressure, measured in mbar, hovers around 1,000. To deal with this, we'll normalize each time series independently, ensuring that they all have small values within a comparable scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXq06b7QWlef"
   },
   "outputs": [],
   "source": [
    "mean = raw_data[: num_train_samples].mean(axis=0)\n",
    "raw_data -= mean\n",
    "std = raw_data[: num_train_samples].std(axis=0)\n",
    "raw_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiQElr1TZ5lG"
   },
   "source": [
    "Next, we'll construct a `dataset` object that produces batches of data over the past five days, along with a target temperature for 24 hours into the future.\n",
    "\n",
    "As before, we could write a `Python` generator to do this, however, `Keras`  provides a handy built-in dataset utility (`timeseries_dataset_from_array()`). As expected, we'll use this utility to instantiate three datasets: one fore training, onf for validation, and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECEUWOdjWlef"
   },
   "outputs": [],
   "source": [
    "sampling_rate = 6\n",
    "sequence_length = 120\n",
    "delay = sampling_rate * (sequence_length + 24 - 1)\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    raw_data[:-delay],\n",
    "    targets=temperature[delay:],\n",
    "    sampling_rate=sampling_rate,\n",
    "    sequence_length=sequence_length,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    start_index=0,\n",
    "    end_index=num_train_samples)\n",
    "\n",
    "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    raw_data[:-delay],\n",
    "    targets=temperature[delay:],\n",
    "    sampling_rate=sampling_rate,\n",
    "    sequence_length=sequence_length,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    start_index=num_train_samples,\n",
    "    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    raw_data[:-delay],\n",
    "    targets=temperature[delay:],\n",
    "    sampling_rate=sampling_rate,\n",
    "    sequence_length=sequence_length,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    start_index=num_train_samples + num_val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsWTAifeWlef"
   },
   "source": [
    "Every dataset provides a tuple (`samples`, `targets`), where samples represent a batch of 256 samples. Each `sample` comprises 120 consecutive hours of input data, while `targets` consist of the corresponding array of 256 target temperatures. It's important to note that the samples are shuffled randomly, meaning that two consecutive sequences in a batch (such as `samples[0]` and `samples[1]`) may not be temporally adjacent.\n",
    "\n",
    "## A common-sense baseline\n",
    "\n",
    "In this scenario, we can reasonably assume that the temperature timeseries is continuous (tomorrow's temperatures are expected to be similar to today's) and periodic, with a daily cycle. Therefore, a straightforward approach is to predict that the temperature 24 hours from now will be the same as the current temperature. We'll assess this method using the mean absolute error (MAE) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VLZ2ZpdWleg",
    "outputId": "b12b5430-fa30-455b-f5d0-a4633f55eb99"
   },
   "outputs": [],
   "source": [
    "def evaluate_naive_method(dataset):\n",
    "\n",
    "    total_abs_err = 0.\n",
    "    samples_seen = 0\n",
    "\n",
    "    for samples, targets in dataset:\n",
    "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
    "        total_abs_err += np.sum(np.abs(preds - targets))\n",
    "        samples_seen += samples.shape[0]\n",
    "\n",
    "    return total_abs_err / samples_seen\n",
    "\n",
    "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
    "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCxnzB4JWleg"
   },
   "source": [
    "This simple baseline results in a validation MAE of 2.44 degrees Celsius and a test MAE of 2.62 degrees Celsius. Therefore, predicting that the temperature 24 hours ahead will be identical to the current temperature leads to an average error of around two and a half degrees. This said, we'd expect our models to do better than this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLgl4WDWWleg"
   },
   "source": [
    "## 1D convolutional model\n",
    "\n",
    "Regarding harnessing appropriate architectural priors, given that our input sequences exhibit daily cycles, it's worth considering a convolutional model. A temporal convolutional network could effectively reuse representations across different days, similar to how a spatial convolutional network can reuse representations across distinct locations in an image.\n",
    "\n",
    "Most likely, if you have worked with images and neural networks, you're already familiar with `Conv2D` and `SeparableConv2D` layers, which process inputs using small sliding windows over 2D grids. Additionally, there are 1D and even 3D versions of these layers: `Conv1D`, `SeparableConv1D`, and `Conv3D`. The `Conv1D` layer operates with 1D sliding windows over input sequences, while the `Conv3D` layer employs cubic windows across input volumes.\n",
    "\n",
    "Following the *translation invariance assumption*, it makes sense to construct 1D convolutional networks, analogous to their 2D counterparts, which are well-suited for sequence data . This assumption implies that the properties within a window remain consistent regardless of its position within the sequence.\n",
    "\n",
    "We'll begin with an initial window length of 24, enabling us to analyze 24 hours of data in each cycle. As we downsample the sequences using `MaxPooling1D` layers, we'll adjust the window size accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwD7U8qpWleg",
    "outputId": "1968b8b9-e5f9-41e2-b512-ec7388c1e800"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.Conv1D(8, 24, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(8, 12, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(8, 6, activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"jena_conv.keras\", save_best_only=True)]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"jena_conv.keras\")\n",
    "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "y76ZdPNuWleg",
    "outputId": "75b2e9a1-23b0-43c2-afcd-03d7a1d5b6ee"
   },
   "outputs": [],
   "source": [
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zspc1saWleh"
   },
   "source": [
    "Unfortunately, this model only achieves a validation MAE of approximately 2.9 degrees, which is significantly higher than the common-sense baseline. Two main factors are causing this result:\n",
    "\n",
    "- Firstly, weather data doesn't adhere well to the translation invariance assumption. Although it exhibits daily cycles, the characteristics of morning data differ from those of evening or night time data. Weather data only exhibits translation invariance on a specific timescale.\n",
    "\n",
    "- Secondly, the order of our data is crucial. Recent data is much more informative for predicting the temperature of the following day compared to data from several days prior. A 1D convolutional network is unable to leverage this importance of chronological order. Specifically, our max pooling and global average pooling layers are largely eliminating crucial order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSXYJueWWleh"
   },
   "source": [
    "## A simple LSTM-based model\n",
    "\n",
    "As you may be aware, there exists a category of neural network architectures tailored for sequential data, known as *Recurrent Neural Networks* (RNNs). Within this family, the *Long Short Term Memory* (LSTM) layer has gained considerable popularity over time. Let's experiment with incorporating the LSTM layer into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeJwuSGgWleh",
    "outputId": "2f1182f6-1edf-44fe-958f-7a8a0f9c06ed"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(16)(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\", save_best_only=True)]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"jena_lstm.keras\")\n",
    "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "FaTszQSCZ5lI",
    "outputId": "bbcd0c9f-6824-4e39-be47-f1d456ed64bf"
   },
   "outputs": [],
   "source": [
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWHyUf8UWleh"
   },
   "source": [
    "We managed to reach a validation MAE of about 2.36 degrees and a test MAE of 2.55 degrees approximately. The LSTM-based approach outperforms the common-sense baseline, although the improvement is modest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O85Dsef0Wlei"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Chollet, Francois. *Deep learning with Python*. Simon and Schuster, 2021."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
